"use strict";(self.webpackChunktrinity_validator=self.webpackChunktrinity_validator||[]).push([[7215],{1183:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var s=t(5893),r=t(1151);const a={title:"Alert Manager",description:"An Alert system for Prometheus",sidebar_position:3},o="Alert Manager",i={id:"sentinelguides/monitoring/alertmanager",title:"Alert Manager",description:"An Alert system for Prometheus",source:"@site/docs/sentinelguides/monitoring/alertmanager.md",sourceDirName:"sentinelguides/monitoring",slug:"/sentinelguides/monitoring/alertmanager",permalink:"/docs/sentinelguides/monitoring/alertmanager",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Alert Manager",description:"An Alert system for Prometheus",sidebar_position:3},sidebar:"monitoringSidebar",previous:{title:"Cosmos Node Exporter",permalink:"/docs/sentinelguides/monitoring/exporters/cosmos-node-exporter"},next:{title:"Prometheus",permalink:"/docs/sentinelguides/monitoring/prometheus"}},l={},c=[{value:"Download &amp; Installation",id:"download--installation",level:2},{value:"Set up the Config file",id:"set-up-the-config-file",level:2},{value:"Create Alerting Rules",id:"create-alerting-rules",level:2},{value:"Edit Prometheus Config File",id:"edit-prometheus-config-file",level:2},{value:"Add a system unit file",id:"add-a-system-unit-file",level:2},{value:"Start Alert Manager service",id:"start-alert-manager-service",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.a)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"alert-manager",children:"Alert Manager"}),"\n",(0,s.jsx)(n.p,{children:"Alerting with Prometheus is separated into two parts."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Alerting rules in Prometheus servers send alerts to an Alertmanager."}),"\n",(0,s.jsx)(n.li,{children:"The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, on-call notification systems, and chat platforms. In this guide, we will employ Telegram as our chosen notification method."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"download--installation",children:"Download & Installation"}),"\n",(0,s.jsxs)(n.p,{children:["To get started, begin by downloading the most recent ",(0,s.jsx)(n.a,{href:"https://github.com/prometheus/alertmanager/releases",children:"release"}),". Once the download is complete, proceed to unzip the file, and you'll be all set to proceed."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"wget https://github.com/prometheus/alertmanager/releases/download/v0.26.0/alertmanager-0.26.0.linux-amd64.tar.gz\ntar xvfz alertmanager-0.26.0.linux-amd64.tar.gz\nsudo rm -f alertmanager-0.26.0.linux-amd64.tar.gz\nmv alertmanager-0.26.0.linux-amd64/ alertmanager/\ncd alertmanager/\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Add a symbolic link to the ",(0,s.jsx)(n.code,{children:"/usr/local/bin/"})," directory for system-wide access to Alert Manager:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo ln -s /home/<your_user>/alertmanager/alertmanager /usr/local/bin/\n"})}),"\n",(0,s.jsx)(n.h2,{id:"set-up-the-config-file",children:"Set up the Config file"}),"\n",(0,s.jsx)(n.p,{children:"The configuration file specifies the recipients to whom alert notifications will be sent. In this example, alerts will be directed to Telegram."}),"\n",(0,s.jsx)(n.p,{children:"Open the config file"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo nano alertmanager.yml\n"})}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"alertmanager.yml"}),(0,s.jsx)("p",{children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"route:\n  receiver: prom_alert_manager_bot\n  repeat_interval: 1h\n\nreceivers:\n  - name: prom_alert_manager_bot\n    telegram_configs:\n      - send_resolved: true\n        bot_token: <your_bot_token>\n        api_url: https://api.telegram.org\n        chat_id: <you_chat_id>\n        parse_mode: ''\n"})})})]}),"\n",(0,s.jsx)(n.h2,{id:"create-alerting-rules",children:"Create Alerting Rules"}),"\n",(0,s.jsxs)(n.p,{children:["To set up alerting rules, create a subfolder named ",(0,s.jsx)(n.code,{children:"rules"}),". This designated folder will function as the centralized repository for managing all your alert rules."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo mkdir rules\ncd rules\n"})}),"\n",(0,s.jsx)(n.p,{children:"Now, for each previously installed exporter, generate the corresponding rule file and populate it with the necessary information."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo nano node-exporter.yml\nsudo nano tendermint-internal-metrics.yml\nsudo nano cosmos-node-exporter.yml\nsudo nano cosmos-validator-exporter.yml\n"})}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"node-exporter.yml"}),(0,s.jsx)("p",{children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="/home/<your_user>/alertmanager/rules/node-exporter.yml',children:'groups:\n  - name: HostAlerts\n    rules:\n      - alert: HostOutOfMemory\n        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host out of memory (instance {{ $labels.instance }})\n          description: "Node memory is filling up (< 10% left)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostMemoryUnderMemoryPressure\n        expr: rate(node_vmstat_pgmajfault[1m]) > 1000\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host memory under memory pressure (instance {{ $labels.instance }})\n          description: "The node is under heavy memory pressure. High rate of major page faults\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostUnusualNetworkThroughputIn\n        expr: sum by (instance, host) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host unusual network throughput in (instance {{ $labels.instance }})\n          description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostUnusualNetworkThroughputOut\n        expr: sum by (instance, host) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host unusual network throughput out (instance {{ $labels.instance }})\n          description: "Host network interfaces are probably sending too much data (> 100 MB/s)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostUnusualDiskReadRate\n        expr: sum by (instance, host, device) (rate(node_disk_read_bytes_total[10m])) / 1024 / 1024 > 50\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host unusual disk read rate (instance {{ $labels.instance }})\n          description: "Disk is probably reading too much data (> 50 MB/s)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostUnusualDiskWriteRate\n        expr: sum by (instance, host, device) (rate(node_disk_written_bytes_total[10m])) / 1024 / 1024 > 50\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host unusual disk write rate (instance {{ $labels.instance }})\n          description: "Disk is probably writing too much data (> 50 MB/s)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostOutOfDiskSpace\n        expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host out of disk space (instance {{ $labels.instance }})\n          description: "Disk is almost full (< 10% left)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostUnusualDiskReadLatency\n        expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host unusual disk read latency (instance {{ $labels.instance }})\n          description: "Disk latency is growing (read operations > 100ms)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostUnusualDiskWriteLatency\n        expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host unusual disk write latency (instance {{ $labels.instance }})\n          description: "Disk latency is growing (write operations > 100ms)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostHighCpuLoad\n        expr: sum by (instance, host) (avg by (mode, instance, host) (rate(node_cpu_seconds_total{mode!="idle"}[2m]))) * 100 > 80\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host high CPU load (instance {{ $labels.instance }})\n          description: "CPU load is > 80%:  {{ $value }}"\n\n      - alert: HostCpuStealNoisyNeighbor\n        expr: avg by(instance, host) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host CPU steal noisy neighbor (instance {{ $labels.instance }})\n          description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostContextSwitching\n        expr: (rate(node_context_switches_total[5m])) / count without (cpu) (node_cpu_frequency_max_hertz) > 10000\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host context switching (instance {{ $labels.instance }})\n          description: "Context switching is growing on node (> 10000 / s)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostSystemdServiceCrashed\n        expr: node_systemd_unit_state{state="failed"} == 1\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host systemd service crashed (instance {{ $labels.instance }})\n          description: "systemd service crashed\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostPhysicalComponentTooHot\n        expr: node_hwmon_temp_celsius > 75\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host physical component too hot (instance {{ $labels.instance }})\n          description: "Physical hardware component too hot\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostNodeOvertemperatureAlarm\n        expr: node_hwmon_temp_crit_alarm_celsius == 1\n        for: 0m\n        labels:\n          severity: critical\n        annotations:\n          summary: Host node overtemperature alarm (instance {{ $labels.instance }})\n          description: "Physical node temperature alarm triggered\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostOomKillDetected\n        expr: increase(node_vmstat_oom_kill[1m]) > 0\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host OOM kill detected (instance {{ $labels.instance }})\n          description: "OOM kill detected\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostEdacCorrectableErrorsDetected\n        expr: increase(node_edac_correctable_errors_total[1m]) > 0\n        for: 0m\n        labels:\n          severity: info\n        annotations:\n          summary: Host EDAC Correctable Errors detected (instance {{ $labels.instance }})\n          description: "Host {{ $labels.instance }} has had {{ printf \\"%.0f\\" $value }} correctable memory errors reported by EDAC in the last 5 minutes.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostEdacUncorrectableErrorsDetected\n        expr: node_edac_uncorrectable_errors_total > 0\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host EDAC Uncorrectable Errors detected (instance {{ $labels.instance }})\n          description: "Host {{ $labels.instance }} has had {{ printf \\"%.0f\\" $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostNetworkReceiveErrors\n        expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host Network Receive Errors (instance {{ $labels.instance }})\n          description: "Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \\"%.0f\\" $value }} receive errors in the last two minutes.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostNetworkTransmitErrors\n        expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host Network Transmit Errors (instance {{ $labels.instance }})\n          description: "Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \\"%.0f\\" $value }} transmit errors in the last two minutes.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostNetworkInterfaceSaturated\n        expr: (rate(node_network_receive_bytes_total{device!~"^tap.*"}[1m]) + rate(node_network_transmit_bytes_total{device!~"^tap.*"}[1m])) / node_network_speed_bytes{device!~"^tap.*"} > 0.8 < 10000\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host Network Interface Saturated (instance {{ $labels.instance }})\n          description: "The network interface \\"{{ $labels.device }}\\" on \\"{{ $labels.instance }}\\" is getting overloaded.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostNetworkBondDegraded\n        expr: (node_bonding_active - node_bonding_slaves) != 0\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host Network Bond Degraded (instance {{ $labels.instance }})\n          description: "Bond \\"{{ $labels.device }}\\" degraded on \\"{{ $labels.instance }}\\".\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostConntrackLimit\n        expr: node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host conntrack limit (instance {{ $labels.instance }})\n          description: "The number of conntrack is approaching limit\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostClockSkew\n        expr: (node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host clock skew (instance {{ $labels.instance }})\n          description: "Clock skew detected. Clock is out of sync. Ensure NTP is configured correctly on this host.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostClockNotSynchronising\n        expr: min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: Host clock not synchronising (instance {{ $labels.instance }})\n          description: "Clock not synchronising. Ensure NTP is configured on this host.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n      - alert: HostRequiresReboot\n        expr: node_reboot_required > 0\n        for: 4h\n        labels:\n          severity: info\n        annotations:\n          summary: Host requires reboot (instance {{ $labels.instance }})\n          description: "{{ $labels.instance }} requires a reboot.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n'})})})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"tendermint-internal-metrics.yml"}),(0,s.jsx)("p",{children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="/home/<your_user>/alertmanager/rules/tendermint-internal-metrics.yml',children:'groups:\n  - name: TendermintInternalAlerts\n    rules:\n      - alert: TendermintInternalInsufficientPeersConnected\n        expr: tendermint_p2p_peers < 3\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: Tendermint node not having enough peers connected (instance {{ $labels.instance }})\n          description: "Tendermint node not having enough peers connected (< 3): {{ $value }}"\n\n      - alert: TendermintInternalNoPeersConnected\n        expr: tendermint_p2p_peers == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: Tendermint node not connected to any peers (instance {{ $labels.instance }})\n          description: "Tendermint node not connected to any peers"\n\n      - alert: TendermintInternalMempoolOverflow\n        expr: tendermint_mempool_size > 100\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: Tendermint node has too many txs in mempool (instance {{ $labels.instance }})\n          description: "Tendermint node has too many txs in mempool: {{ $value }}"\n'})})})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"cosmos-node-exporter.yml"}),(0,s.jsx)("p",{children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="/home/<your_user>/alertmanager/rules/cosmos-node-exporter.yml',children:'groups:\n  - name: CosmosNodeExporter\n    rules:\n\n      - alert: CosmosNodeCatchingUp\n        expr: cosmos_node_exporter_catching_up == 1\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: Tendermint node not in sync (host {{ $labels.host }})\n          description: "Tendermint node not in sync"\n\n      - alert: CosmosNodeNotLatestBinary\n        expr: cosmos_node_exporter_is_latest == 0 AND on (instance, host) cosmos_node_exporter_upgrade_coming == 0\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Tendermint node is not running the latest binary (host {{ $labels.host }}): github version {{ $labels.remote_version }}, local version: {{ $labels.local_version }})"\n          description: "Tendermint node is not running the latest binary"\n\n      - alert: CosmosNodeVotingPowerValidator\n        expr: cosmos_node_exporter_voting_power{type="validator"} == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: "Tendermint validator node has 0 voting power (host {{ $labels.host }})"\n          description: "Tendermint validator node has 0 voting power"\n\n      - alert: CosmosNodeVotingPowerTestnet\n        expr: cosmos_node_exporter_voting_power{type="testnet"} == 0\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Tendermint testnet node has 0 voting power (host {{ $labels.host }}"\n          description: "Tendermint testnet node has 0 voting power"\n\n      - alert: CosmosNodeTimeSinceLatestBlock\n        expr: (cosmos_node_exporter_time_since_latest_block / 60) > 10\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: "Tendermint node\'s latest block was later than 10 minutes ago (host {{ $labels.host }}"\n          description: "Tendermint node\'s latest block was later than 10 minutes ago \\n  VALUE = {{ $value }}"\n\n      - alert: CosmosNodeUpgradeBinaryNotPresent\n        expr: cosmos_node_exporter_upgrade_binary_present == 0\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Cosmos node does not have the upgrade binary (host {{ $labels.host }}, upgrade {{ $labels.name }})"\n          description: "Cosmos node does not have the upgrade binary"\n\n      - alert: CosmosNodeErrors\n        expr: cosmos_node_exporter_query_successful == 0\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "cosmos-node-exporter has errors (host {{ $labels.host }})"\n          description: "cosmos-node-exporter has errors"\n\n      - alert: CosmosNodeUpgradeUpcoming\n        expr: (cosmos_node_exporter_upgrade_estimated_time - time()) < 30 * 60\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: "There\'s an upgrade upcoming on node {{ $labels.host }}: {{ $labels.name }} in {{ $value }} seconds"\n          description: "There\'s an upgrade upcoming"\n'})})})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"cosmos-validator-exporter"}),(0,s.jsx)("p",{children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="/home/<your_user>/alertmanager/rules/cosmos-validator-exporter.yml',children:'groups:\n  - name: CosmosValidatorsExporterAlerts\n    rules:\n      - alert: CosmosValidatorsExporterNodeError\n        expr: cosmos_validators_exporter_queries_error > 0\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: cosmos-validators-exporter has some errors from chain (instance {{ $labels.instance }}, chain {{ $labels.chain }})\n          description: "cosmos-validators-exporter has some errors from chain"\n\n      - alert: CosmosValidatorsExporterValidatorAtLastPlace\n        expr: cosmos_validators_exporter_validators_count - cosmos_validators_exporter_rank <= 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: Cosmos validator is at the last place (instance {{ $labels.instance }})\n          description: "Cosmos validator is at the last place \\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n'})})})]}),"\n",(0,s.jsx)(n.h2,{id:"edit-prometheus-config-file",children:"Edit Prometheus Config File"}),"\n",(0,s.jsxs)(n.p,{children:["Inside your ",(0,s.jsx)(n.code,{children:"prometheus"})," directory open the ",(0,s.jsx)(n.code,{children:"prometheus.yml"})," file:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo nano prometheus.yml\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Add the following code block with the ",(0,s.jsx)(n.code,{children:"target"})," and the created ",(0,s.jsx)(n.code,{children:"rule files"}),":"]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"prometheus.yml"}),(0,s.jsx)("p",{children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="/home/<your_user>/prometheus/prometheus.yml',children:'# Alertmanager configuration\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n\nrule_files:\n  - "rules/node-exporter.yml"\n  - "rules/tendermint-internal-metrics.yml"\n  - "rules/cosmos-node-exporter.yml"\n  - "rules/cosmos-validator-exporter.yml"\n'})})})]}),"\n",(0,s.jsx)(n.h2,{id:"add-a-system-unit-file",children:"Add a system unit file"}),"\n",(0,s.jsx)(n.p,{children:"Open the .service with a text editor"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo nano /etc/systemd/system/alertmanager.service\n"})}),"\n",(0,s.jsx)(n.p,{children:"Paste the below text"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"alertmanager.service"}),(0,s.jsx)("p",{children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",metastring:'title="/etc/systemd/system/alertmanager.service"',children:"[Unit]\nDescription=Alert Manager\nAfter=network-online.target\n\u200b\n[Service]\nUser=<your_user> #modify this field with your user\nTimeoutStartSec=0\nCPUWeight=95\nIOWeight=95\nExecStart=alertmanager --config.file=/home/<your_user>/alertmanager/alertmanager.yml --storage.path=/home/<your_user>/alertmanager/data\nRestart=always\nRestartSec=2\nLimitNOFILE=800000\nKillSignal=SIGTERM\n\u200b\n[Install]\nWantedBy=multi-user.target\n"})})})]}),"\n",(0,s.jsx)(n.p,{children:"Reload the systemd Daemon"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo systemctl daemon-reload\n"})}),"\n",(0,s.jsx)(n.p,{children:"Enable autostart of Node Exporter service"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo systemctl enable alertmanager.service\n"})}),"\n",(0,s.jsx)(n.h2,{id:"start-alert-manager-service",children:"Start Alert Manager service"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo systemctl start alertmanager.service\n"})}),"\n",(0,s.jsx)(n.p,{children:"Use this command to check logs in real time"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo journalctl -u alertmanager.service -f\n"})}),"\n",(0,s.jsx)(n.p,{children:"After installing and running Alert Manager, you can verify whether alerts are being displayed."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"curl -u admin http://localhost:9093/metrics\n"})}),"\n",(0,s.jsx)(n.p,{children:"You can now enter this address in your browser to check if Prometheus displays them."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"https://prometheus_ip:9090/alerts\n"})})]})}function m(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>i,a:()=>o});var s=t(7294);const r={},a=s.createContext(r);function o(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);